Backpropagation algorithm for Neural Network training
===========

This is the week 4 assignment of [Coursera Machine Learning] (https://www.coursera.org/course/ml) class.

Summary
------
In this program, the backpropagation algorithm is implemented for a 3-layered neural networks and applied to the task of hand-written digit recognition. After feedforward step of the neural network that returns the cost function, backpropagation algorithm is used to compute the gradients for the parameters for the regularized neural network.

Completed methods are as follows:

sigmoidGradient.m - Compute the gradient of the sigmoid function <br\ >
randInitializeWeights.m - Randomly initialize weights <br\ >
nnCostFunction.m - Neural network cost function <br\ >

Starter code and data sets provided by the course is shown below:
> ex4.m - script for stepping through the program <br\ >
> ex4data1.mat - Training set of hand-written digits <br\ >
> ex4weights.mat - Neural network parameters for exercise 4 <br\ >
> displayData.m - Function to help visualize the dataset <br\ >
> fmincg.m - Function minimization routine <br\ >
> sigmoid.m - Sigmoid function <br\ >
> computeNumericalGradient.m - Numerically compute gradients <br\ >
> checkNNGradients.m - Function to help check gradients <br\ >
> debugInitializeWeights.m - Function for initializing weights <br\ >
> predict.m - Neural network prediction function <br\ >

